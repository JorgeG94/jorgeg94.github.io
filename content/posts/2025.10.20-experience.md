+++
title = "Going from C++ to Fortran (1/n)"
date = "2025-10-19T17:01:00"
author = "Jorge"
#cover = "img/hello.jpg"
description = "How did I get started"
+++

## Intro

My programming experience has been a bit unconventional. I learned how to program in order to 
do my PhD in Computational Chemistry, back in between 2017 and 2022 at Iowa State University.

However, the journey started by asking "what should I learn?", since my experience with 
programming was extremely limited. For the summer internship I did at the same group
I very happily took a course on how to use the command line and bash to do scripting. 

My first programs were actually playing around with bash and processing files: moving them 
from one place to another, copying them, doing grep, sed to change something, etc. So, nothing
really too fancy. I didn't know too much about testing and making sure things worked everywhere.

I then received my first project for my PhD, which would last my entire PhD: evaluation of 
two electron integrals over Gaussian basis functions. Cool stuff, very computationally
demanding, a bit complicated to solve. Therefore, I realized I needed to learn 
"proper" programming. The problem is that I did not know where to start, not even how to start.

I naively asked older graduate students, failing to think that they would've probably been in the 
exact same position as me. "Don't learn from the codebase", I was told multiple times. "Read these 
books", as I was given the "Modern C++" books, and a bunch of similar references. "Do project Euler" to
practice once you've learned. 

I got a lot of advise, but not really any solid advise on what is considered learning. For chemistry and
other physical sciences, it is quite simple to realize if you've learned something, because you "know" it, 
you're able to explain it without much problems. But since programming is a much more creative endeavour, 
I believe it is actually quite difficult to show that you've learned it, since at some point it boils down
to personal preference for a lot of things. 

This hodge podge of things lead me to craft a journey of my own. I thought, well I know about Python and that 
it is a simple language and pretty well documented. I shall start by coding up these integrals in Python. 

In the year of 2017, there were less options for IDE, or at least less free ones. You had Atom, which I believe
no longer exists, SublimeText2, Notepad++, VStudio, Vim, EMacs, among others. I learned vim because that's 
what was installed on the remote clusters I was using. I didn't know that you could have `vimrc` files to 
dictate the number of spaces, tabs, etc. So eventually, my vim written python code was riddled with "incosistent use
of tabs and spaces". I believe this is the reason I thoroughly dislike Python. 

After I learned to use Python, I wrote a pretty cool small Self Consistent Field (SCF) program. It was horrendously
slow, but it worked and gave correct answers. 

My project would involve writing integrals that could run on GPUs. I did not know what GPUs were. One more question 
to the collection! I was told "You should learn C++". "How do I do that?" I replied. Someone shared with me a 9 hour
vide by a person called Bucky. He went through a lot of the basics of C++ and I took notes and have a bunch of 
stupid programs that print out "I am a salmon", "I am a trout", using some interesting concepts of object orientation, 
among other stuff. 

This, however, did not teach me how to code. I learned to speak C++ but I was not told why certain things 
were stupid and why certain other things were smart. "what happens if I forget to free my memory?", "A class should
hopefully do one thing". "Data encapsulation is good", etc. I just learned how to do stuff and was quickly writing some
pretty bad code using (overusing) `std::vector` and a collection of many many stupid things. 

"Is this passed by reference?"..."let me just add a `*` to the function signature...hmm, maybe a &?...no, `variable[]`"
This was how I coded and I wrote some pretty horrific code. 

No one told me that code should be readable by humans. Why should humans read my code? The machine is. This is 
a very stupid thing to think. One should write code for humans to read and for computers to consume.

During my graduate school career I learned to cook. I now envision programming as cooking. Perhaps you have noticed 
this, if you've read other posts here. This has lead me to write much nicer code, specially more maintainable and 
reusable code. 



